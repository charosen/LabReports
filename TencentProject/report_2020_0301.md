#  腾讯项目组汇报（四十八）--2020/03/01

<h2>1. 多进程DatasetReader</h2>

nlp tagger对句子标注依存树比较耗时，导致一次数据读取及dependecy tree预处理会消耗较长时间，因此，我尝试将Datase


最近这几天把论文Byte Pair Dependency Tree的构建代码编写完成

```
from typing import Dict, List, Tuple, Set
import json
import tarfile
import re
import logging

from overrides import overrides

from allennlp.common.checks import ConfigurationError
from allennlp.common.file_utils import cached_path
from allennlp.common.util import pad_sequence_to_length
from allennlp.data.vocabulary import Vocabulary
from allennlp.data.tokenizers.token import Token
from allennlp.data.token_indexers.token_indexer import TokenIndexer

logger = logging.getLogger(__name__)

def text_standardize(text):
    """
    Apply text standardization following original implementation.
    """
    # pylint: disable=anomalous-backslash-in-string
    text = text.replace('—', '-')
    text = text.replace('–', '-')
    text = text.replace('―', '-')
    text = text.replace('…', '...')
    text = text.replace('´', "'")
    text = re.sub('''(-+|~+|!+|"+|;+|\?+|\++|,+|\)+|\(+|\\+|\/+|\*+|\[+|\]+|}+|{+|\|+|_+)''', r' \1 ', text)
    text = re.sub('\s*\n\s*', ' \n ', text)
    text = re.sub('[^\S\n]+', ' ', text)
    return text.strip()


@TokenIndexer.register("byte_pair_dep_tree_indexer")
class BytePairDepTreeIndexer(TokenIndexer[int]):
    # TODO: change comment
    """
    Generates the indices for the byte-pair encoding used by
    the OpenAI transformer language model: https://blog.openai.com/language-unsupervised/
    This is unlike most of our TokenIndexers in that its
    indexing is not based on a `Vocabulary` but on a fixed
    set of mappings that are loaded by the constructor.
    Note: recommend using ``OpenAISplitter`` tokenizer with this indexer,
    as it applies the same text normalization as the original implementation.
    Note 2: when ``tokens_to_add`` is not None, be sure to set
    ``n_special=len(tokens_to_add)`` in ``OpenaiTransformer``, otherwise
    behavior is undefined.
    """
    # pylint: disable=no-self-use
    def __init__(self,
                 encoder: Dict[str, int] = None,
                 byte_pairs: List[Tuple[str, str]] = None,
                 n_ctx: int = 512,
                 model_path: str = None,
                 namespace: str = 'dep_labels',
                 tokens_to_add: List[str] = None) -> None:
        self._namespace = namespace
        self._logged_errors: Set[str] = set()
        # self._added_to_vocabulary = False

        # TODO: avoid duplicate loading encoder & bpe_ranks
        too_much_information = model_path and (encoder or byte_pairs)
        too_little_information = not model_path and not (encoder and byte_pairs)

        if too_much_information or too_little_information:
            raise ConfigurationError("must specify either model path or (encoder + byte_pairs) but not both")

        if model_path:
            model_path = cached_path(model_path)

            # Load encoder and byte_pairs from tar.gz
            with tarfile.open(model_path) as tmp:
                encoder_name = next(m.name for m in tmp.getmembers() if 'encoder_bpe' in m.name)
                encoder_info = tmp.extractfile(encoder_name)

                if encoder_info:
                    encoder = json.loads(encoder_info.read())
                else:
                    raise ConfigurationError(f"expected encoder_bpe file in archive {model_path}")

                bpe_name = next(m.name for m in tmp.getmembers() if m.name.endswith('.bpe'))
                bpe_info = tmp.extractfile(bpe_name)

                if bpe_info:
                    # First line is "version", last line is blank
                    lines = bpe_info.read().decode('utf-8').split('\n')[1:-1]
                    # Convert "b1 b2" -> (b1, b2)
                    byte_pairs = [tuple(line.split()) for line in lines]  # type: ignore
                else:
                    raise ConfigurationError(f"expected .bpe file in archive {model_path}")

        if tokens_to_add is not None:
            for token in tokens_to_add:
                encoder[token + '</w>'] = len(encoder)
            self.tokens_to_add = set(tokens_to_add)
        else:
            self.tokens_to_add = None

        self.encoder = encoder
        self.decoder = {word_id: word for word, word_id in self.encoder.items()}

        # Compute ranks
        self.bpe_ranks = {pair: idx for idx, pair in enumerate(byte_pairs)}

        self.cache: Dict[str, List[str]] = {}
        self.n_ctx = n_ctx


    @overrides
    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
        # Here we count dependency edge labels, and construct vocab for them.
        dep_label = token.dep_
        if not dep_label:
            if token.text not in self._logged_errors:
                logger.warning("Token had no dependency label: %s", token.text)
                self._logged_errors.add(token.text)
        elif dep_label != "ROOT":
            counter[self._namespace][dep_label] += 1

        counter[self._namespace]["self"] += 1


    def get_dep_edges(self,
                      h_token: Token,
                      t_tok_head: int,
                      i_offset: int,
                      offsets: List[int],
                      vocabulary: Vocabulary) -> Dict[Tuple[int, int], int]:
        # TODO: add comment
        temp_dict = {}
        # DEBUG sentence
        # if h_token.dep_ == "ROOT" and t_tok_head < self.n_ctx-2:  # doesn't count oor roots
        #     self.num_roots += 1
        if h_token.dep_ and h_token.dep_ != "ROOT":
            # Get dependent edge
            t_tok_tail = offsets[h_token.head.i + i_offset]
            dep_id = vocabulary.get_token_index(h_token.dep_, self._namespace)
            temp_dict.update({(t_tok_head, t_tok_tail): dep_id})
            # DEBUG pruning sentence
            # if t_tok_head < self.n_ctx-2 and t_tok_tail >= self.n_ctx-2:
            #     self.oor_edges += 1
        # Get self-loop edge
        self_id = vocabulary.get_token_index("self", self._namespace)
        temp_dict.update({(t_tok_head, t_tok_head): self_id})

        try:
            children_generator = h_token.children  # __start__, __del1__, ... will raise error
        except:
            return temp_dict
        # Get governor edges, that is, inverse edges
        for t_token in children_generator:
            # chilren include root?
            if t_token.dep_ == "ROOT":
                raise ValueError("Children contains roots!!!")
            t_tok_tail = offsets[t_token.i + i_offset]
            dep_id = vocabulary.get_token_index(t_token.dep_, self._namespace)
            temp_dict.update({(t_tok_head, t_tok_tail): dep_id})

        return temp_dict


    def byte_pair_encode(self, token: Token, lowercase: bool = True) -> List[str]:
        if lowercase:
            text = token.text.lower()
        else:
            text = token.text

        if text in self.cache:
            return self.cache[text]

        if self.tokens_to_add and text in self.tokens_to_add:
            # this is a special token, and it's guaranteed to be a word
            word = [text + '</w>']
            self.cache[text] = word
            return word

        # Split into letters, but add a `</w>` to the last
        word = [c for c in text[:-1]]
        word.append(text[-1] + '</w>')

        # Get unique pairs (prev_symbol, next_symbol)
        pairs = {(prev_symbol, next_symbol)
                 for prev_symbol, next_symbol in zip(word, word[1:])}

        if not pairs:
            return [text + '</w>']

        while True:
            # Find the highest ranked pair
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))

            # If that pair is not actually ranked, stop.
            if bigram not in self.bpe_ranks:
                break

            # Split up the pair
            first, second = bigram

            # and make a helper for a new word
            new_word = []
            i = 0

            # Iterate over the letters of the word
            while i < len(word):
                try:
                    # Find first occurrence of `first` after i,
                    j = word.index(first, i)
                    # add all the characters preceding it,
                    new_word.extend(word[i:j])
                    # and update i to j
                    i = j
                except ValueError:
                    # `first` didn't occur, so just add the rest
                    new_word.extend(word[i:])
                    break  # out of while i < len(word)

                # At this point we know word[i] == first
                if i < len(word) - 1 and word[i + 1] == second:
                    new_word.append(first + second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1

            word = new_word
            if len(word) == 1:
                break  # out of while True
            else:
                pairs = {(prev_symbol, next_symbol)
                         for prev_symbol, next_symbol in zip(word, word[1:])}

        if ' '.join(word) == '\n  </w>':
            word = ['\n</w>']

        self.cache[text] = word
        return word

    # def _add_encoding_to_vocabulary(self, vocabulary: Vocabulary) -> None:
        # pylint: disable=protected-access
        # for word, idx in self.encoder.items():
        #     vocabulary._token_to_index[self._namespace][word] = idx
        #     vocabulary._index_to_token[self._namespace][idx] = word

    @overrides
    def tokens_to_indices(self,
                          tokens: List[Token],
                          vocabulary: Vocabulary,
                          index_name: str) -> Dict[str, List[int]]:
        text_tokens = []
        offsets = []
        offset = -1
        bpe_tokens_list = []

        for token in tokens:
            bpe_tokens = [self.encoder.get(t, 0) for t in self.byte_pair_encode(token) if self.encoder.get(t, 0) != 0]

            if bpe_tokens:
                offset += len(bpe_tokens)
                offsets.append(offset)
                text_tokens.extend(bpe_tokens)
                bpe_tokens_list.append(bpe_tokens)


        dep_edges_dict = {}
        # DEBUG sentence
        # self.num_roots = 0
        # self.oor_edges = 0
        # Get entity & delimiter token offset
        _start_ = bpe_tokens_list.index([self.encoder.get("__start__</w>")])
        _del1_ = bpe_tokens_list.index([self.encoder.get("__del1__</w>")])
        _del2_ = bpe_tokens_list.index([self.encoder.get("__del2__</w>")])

        for tok_pos, bpe_tokens in enumerate(bpe_tokens_list):
            # Get entity or delimiter token offset
            if tok_pos < _del1_:
                i_offset = _start_ + 1
            elif tok_pos < _del2_:
                i_offset = _del1_ + 1
            else:
                i_offset = _del2_ + 1
            # Get the text token offset of the last byte pair of a word
            t_tok_head = offsets[tok_pos]
            token = tokens[tok_pos]
            # Assign the last byte pair of a word with dep edges of its corresponding word
            dep_edges_dict.update(self.get_dep_edges(token,
                                                     t_tok_head,
                                                     i_offset,
                                                     offsets,
                                                     vocabulary))
            # Complete rest byte pair with `compound` edges heading last byte pair
            for rel_pos in range(1, len(bpe_tokens)):
                t_tok_tail = t_tok_head - rel_pos
                dep_id = vocabulary.get_token_index("compound", self._namespace)
                self_id = vocabulary.get_token_index("self", self._namespace)
                dep_edges_dict.update({
                    (t_tok_head, t_tok_tail) : dep_id,
                    (t_tok_tail, t_tok_head) : dep_id,
                    (t_tok_tail, t_tok_tail) : self_id  # self-loop edge
                })
                # DEBUG sentence
                # if t_tok_tail < self.n_ctx-2 and t_tok_head >= self.n_ctx-2:
                #     self.oor_edges += 1

        num_tokens = len(text_tokens)

        # If there's too many tokens, that's going to cause problems.
        if num_tokens >= self.n_ctx:
            # TODO: 存在一个bug，当句子过长时，只截断text_tokens，不截断offsets、mask, 而且截断后
            #  text_tokens, offsets, mask不能相同长度，否则会被allennlp强制转化padding——keys为num——tokens
            #  只有整个batch的第一个instance或所有instances都text_tokens, offsets, mask等长才会报错，如果只有部分instances等长，这些instances
            #  可能会被截短，因为它们的长度并未padding_lengths考虑在内
            #  see (text_field.py line 112)
            print('Sequence to long. Pruning!')
            text_tokens = text_tokens[:self.n_ctx]
            text_tokens[-2] = self.encoder['__clf__</w>']
            text_tokens[-1] = 0
            # Filter all edges out of range [0, 509]
            dep_edges_tuple = [(k, v) for k, v in dep_edges_dict.items()
                               if all(indice < self.n_ctx-2 for indice in k)]
            # Add back a self-loop edge for `__clf__</w>`
            dep_edges_tuple.append(
                ((self.n_ctx-2, self.n_ctx-2),
                        vocabulary.get_token_index("self", self._namespace))
            )
        else:
            # TODO: 末尾补0是为了让text_tokens与offsets, mask长度不一致，否则会被allennlp强制转化padding——keys为num——tokens
            #  只有整个batch的第一个instance或所有instances都text_tokens, offsets, mask等长才会报错，如果只有部分instances等长，这些instances
            #  可能会被截短，因为它们的长度并未padding_lengths考虑在内
            #  see (text_field.py line 112)
            text_tokens.append(0)
            dep_edges_tuple = [(k, v) for k, v in dep_edges_dict.items()]

        dep_edges_tuple.sort(key=lambda x: x[0][0]*(len(text_tokens)-1) + x[0][1])
        dep_indices, dep_values = zip(*dep_edges_tuple)


        return {
                "num_tokens": text_tokens,
                # DEBUG sentence
                # "num_roots": self.num_roots,
                # "num_oors" : self.oor_edges,
                f"{index_name}-indices": dep_indices,
                f"{index_name}-values": dep_values,
        }

    @overrides
    def get_padding_token(self) -> int:
        return 0

    @overrides
    def get_padding_lengths(self, token: int) -> Dict[str, int]:  # pylint: disable=unused-argument
        return {}

    @overrides
    def pad_token_sequence(self,
                           tokens: Dict[str, List[int]],
                           desired_num_tokens: Dict[str, int],
                           padding_lengths: Dict[str, int]) -> Dict[str, List[int]]:  # pylint: disable=unused-argument
        # We use `torch.sparse` matrix, so nothing is done here.
        return tokens
        # return {key: pad_sequence_to_length(val, desired_num_tokens[key])
        #         for key, val in tokens.items()}

```

为检查构建的字节对依存树是否正确，编写并进行的单元测试，自动化检查树的共性特征是否正确（包括边树、对称性等等），经过几次测试与修改已经初步确认代码正确，所有单元测试均通过，代码如下：

```
# pylint: disable=no-self-use,invalid-name,protected-access

from allennlp.common.testing import AllenNlpTestCase
from allennlp.data.vocabulary import Vocabulary


from tre.dataset_readers.reside_dep_nyt_reader import ResideDepNYTReader
from tre.token_indexers.byte_pair_indexer import OpenaiTransformerBytePairIndexer
from tre.token_indexers.byte_pair_dep_indexer import BytePairDepTreeIndexer


class TestBytePairDepTreeIndexer(AllenNlpTestCase):
    def setUp(self):
        super().setUp()


    def _create_experiment(self, model_path, tokens_to_add, data_path):

        self.dep_indexer = BytePairDepTreeIndexer(model_path=model_path,
                                                  tokens_to_add=tokens_to_add)
        self.bp_indexer = OpenaiTransformerBytePairIndexer(model_path=model_path,
                                                           tokens_to_add=tokens_to_add)
        self.reside_reader = ResideDepNYTReader(pos_tags=True,
                                                parse=True,
                                                ner=True,
                                                token_indexers={"byte_pairs": self.bp_indexer},
                                                dep_indexers={"bp_dep_tree": self.dep_indexer})
        self.test_data = self.reside_reader.read(data_path)
        self.vocab = Vocabulary.from_instances(self.test_data)


    def test_tokens_to_indices_one_batch(self):
        MODEL_PATH = "tests/fixtures/openai-finetune-lm.tar.gz"
        TOKENS_TO_ADD = ["__start__",
                         "__del1__",
                         "__del2__",
                         "__clf__",
                         "__mask__"]
        DATA_PATH = "tests/fixtures/reside_nyt/train_one_batch.txt"
        self._create_experiment(MODEL_PATH, TOKENS_TO_ADD, DATA_PATH)

        for ins in self.test_data:
            dep_tree = self.dep_indexer.tokens_to_indices(tokens=ins["dependency"].tokens,
                                                          vocabulary=self.vocab,
                                                          index_name="bp_dep_tree")
            # Check padding keys
            assert set(dep_tree.keys()) == {"num_tokens", "num_roots", "num_oors", "bp_dep_tree-indices", "bp_dep_tree-values"}
            # Check if edges' indices are legal
            num_tokens = len(dep_tree["num_tokens"])
            num_roots = dep_tree["num_roots"]
            num_oors = dep_tree["num_oors"]
            assert all(idx < num_tokens-1 for idxs in dep_tree["bp_dep_tree-indices"] for idx in idxs) == True
            # Check sorting
            t_token_heads = [idxs[0] for idxs in dep_tree["bp_dep_tree-indices"]]
            assert sorted(t_token_heads) == t_token_heads
            # Check indices & values count
            assert len(dep_tree["bp_dep_tree-indices"]) == len(dep_tree["bp_dep_tree-values"])
            # Check duplicate/overlapping indices
            assert len(set(dep_tree["bp_dep_tree-indices"])) == len(dep_tree["bp_dep_tree-indices"])
            # Check total edges count
            try:
                assert len(dep_tree["bp_dep_tree-values"]) == 2 * (((num_tokens-1) - 4) - num_roots) + (num_tokens-1)
            except:
                # check pruning sentence
                assert len(dep_tree["bp_dep_tree-values"]) == 2 * (((num_tokens-1) - 4) - num_roots) + (num_tokens-1) - 2 * num_oors
            # self-loop count should be equal to num_tokens-1
            assert dep_tree["bp_dep_tree-values"].count(self.vocab.get_token_index("self", self.dep_indexer._namespace)) == num_tokens - 1


    def test_tokens_to_indices_RESIDE(self):
        MODEL_PATH = "tests/fixtures/openai-finetune-lm.tar.gz"
        TOKENS_TO_ADD = ["__start__",
                         "__del1__",
                         "__del2__",
                         "__clf__",
                         "__mask__"]
        DATA_PATH = "./data/reside_nyt/train.txt"
        self._create_experiment(MODEL_PATH, TOKENS_TO_ADD, DATA_PATH)

        for ins in self.test_data:
            dep_tree = self.dep_indexer.tokens_to_indices(tokens=ins["dependency"].tokens,
                                                          vocabulary=self.vocab,
                                                          index_name="bp_dep_tree")
            # Check padding keys
            assert set(dep_tree.keys()) == {"num_tokens", "num_roots", "num_oors", "bp_dep_tree-indices", "bp_dep_tree-values"}
            # Check if edges' indices are legal
            num_tokens = len(dep_tree["num_tokens"])
            num_roots = dep_tree["num_roots"]
            num_oors = dep_tree["num_oors"]
            assert all(idx < num_tokens-1 for idxs in dep_tree["bp_dep_tree-indices"] for idx in idxs) == True
            # Check sorting
            t_token_heads = [idxs[0] for idxs in dep_tree["bp_dep_tree-indices"]]
            assert sorted(t_token_heads) == t_token_heads
            # Check indices & values count
            assert len(dep_tree["bp_dep_tree-indices"]) == len(dep_tree["bp_dep_tree-values"])
            # Check duplicate/overlapping indices
            assert len(set(dep_tree["bp_dep_tree-indices"])) == len(dep_tree["bp_dep_tree-indices"])
            # Check total edges count
            try:
                assert len(dep_tree["bp_dep_tree-values"]) == 2 * (((num_tokens-1) - 4) - num_roots) + (num_tokens-1)
            except:
                # check pruning sentence
                assert len(dep_tree["bp_dep_tree-values"]) == 2 * (((num_tokens-1) - 4) - num_roots) + (num_tokens-1) - 2 * num_oors
            # self-loop count should be equal to num_tokens-1
            assert dep_tree["bp_dep_tree-values"].count(self.vocab.get_token_index("self", self.dep_indexer._namespace)) == num_tokens - 1


```

现已初步把后续的数据迭代Iterator写好，正在调试中，以检查模型输入的正确性，代码略，代调试好后附在报告中。

